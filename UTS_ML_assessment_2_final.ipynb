{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "71c1d039",
      "metadata": {
        "id": "71c1d039"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "id": "0cf6f7ae",
      "metadata": {
        "id": "0cf6f7ae"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1197da9f",
      "metadata": {
        "id": "1197da9f"
      },
      "source": [
        "## Classes and Methods "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "id": "4c68d06f",
      "metadata": {
        "id": "4c68d06f"
      },
      "outputs": [],
      "source": [
        "\n",
        "def prepare_data(dataset_1=None, dataset_2=None, dataset_3=None, dataset_4=None):\n",
        "    '''function to prepare the data so it can be used in the classifier'''\n",
        "    #concatenate the 4 seperate datasets to one single dataset\n",
        "    dataset_df = pd.concat([dataset_1, dataset_2, dataset_3, dataset_4])\n",
        "\n",
        "    #seperate attributes from labels\n",
        "    X = dataset_df.iloc[:, :-1].values\n",
        "    Y = dataset_df.iloc[:, -1:].values\n",
        "\n",
        "    #split into training, validation and test data\n",
        "    \n",
        "    X_training, X_test, Y_training, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 11)\n",
        "    X_train, X_val, Y_train, Y_val = train_test_split(X_training, Y_training, test_size = 0.25, random_state = 11) # 0.25 * 0.8 = 0.2\n",
        "    return X_train, X_test, X_val, Y_train, Y_test, Y_val\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7432a575",
      "metadata": {
        "id": "7432a575"
      },
      "source": [
        "### Class for Nodes of the Tree\n",
        "This class only includes the constructor method which assigns values to the variables when a Tree_Node instance is created. Assign default value 'None' because we sometimes dont assign values to all variables on creation. \n",
        "\n",
        "Variables:\n",
        "- attribute_index\n",
        "- splitting_value\n",
        "- left\n",
        "- right\n",
        "- info_gain\n",
        "- value\n",
        "\n",
        "Methods:\n",
        "- constructor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "id": "5ef4ec33",
      "metadata": {
        "id": "5ef4ec33"
      },
      "outputs": [],
      "source": [
        "class Tree_Node():\n",
        "    def __init__(self, attribute_index=None, splitting_value=None, left=None, right=None, info_gain=None, value=None):\n",
        "        ''' constructor ''' \n",
        "        \n",
        "        # attribute_index of the attribute that is used to decide whether to follow the left or right side of the tree\n",
        "        self.attribute_index = attribute_index\n",
        "        # value at which you split into left and right \n",
        "        self.splitting_value = splitting_value\n",
        "        # contains the left side under this node\n",
        "        self.left = left\n",
        "        # contains the right side under this node\n",
        "        self.right = right\n",
        "        # information gain of the node\n",
        "        self.info_gain = info_gain\n",
        "\n",
        "        # label value which will be predicted if the node is a leaf node (majority class)\n",
        "        self.value = value"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be769048",
      "metadata": {
        "id": "be769048"
      },
      "source": [
        "### Class for the Decision Tree Classifier\n",
        "Includes all variables and methods to build a Classification Tree using ID3 algorithm and to make predictions with that tree.\n",
        "\n",
        "Variables:\n",
        "- root\n",
        "- min_samples_split\n",
        "- max_depth\n",
        "\n",
        "Methods:\n",
        "- constructor\n",
        "- build_tree_ID3\n",
        "- find_best_split\n",
        "- split\n",
        "- calc_information_gain\n",
        "- calc_entropy\n",
        "- majority_vote\n",
        "- predict\n",
        "- make_prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "id": "96c4b4fc",
      "metadata": {
        "id": "96c4b4fc"
      },
      "outputs": [],
      "source": [
        "class DecisionTreeClassifier():\n",
        "    def __init__(self, min_samples_split, max_depth):\n",
        "        ''' constructor '''\n",
        "        \n",
        "        # initialize the root node of the tree \n",
        "        self.root = None\n",
        "        \n",
        "        # stopping criteria\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.max_depth = max_depth\n",
        "        \n",
        "    def build_tree_ID3(self, dataset, curr_depth=0):\n",
        "        ''' recursive ID3 algorithm to build the tree ''' \n",
        "        \n",
        "        X, Y = dataset[:,:-1], dataset[:,-1]\n",
        "        num_samples, num_attributes = np.shape(X)\n",
        "        \n",
        "        # split until stopping criteria are met\n",
        "        if num_samples>=self.min_samples_split and curr_depth<=self.max_depth:\n",
        "            # find the best split by calling the corresponding method\n",
        "            best_split = self.find_best_split(dataset, num_samples, num_attributes)\n",
        "            # check if information gain is positive\n",
        "            if best_split[\"info_gain\"]>0:\n",
        "                # call the same function with the left subtree dataset as input\n",
        "                left_subtree = self.build_tree_ID3(best_split[\"dataset_left\"], curr_depth+1)\n",
        "                # call the same function with the right subtree dataset as input\n",
        "                right_subtree = self.build_tree_ID3(best_split[\"dataset_right\"], curr_depth+1)\n",
        "                # return tree node\n",
        "                return Tree_Node(best_split[\"attribute_index\"], best_split[\"splitting_value\"], \n",
        "                            left_subtree, right_subtree, best_split[\"info_gain\"])\n",
        "        \n",
        "        # compute leaf node\n",
        "        leaf_value = self.majority_vote(Y)\n",
        "        # return leaf node \n",
        "        return Tree_Node(value=leaf_value)\n",
        "    \n",
        "    def find_best_split(self, dataset, num_samples, num_attributes):\n",
        "        ''' function to find the best split '''\n",
        "        \n",
        "        # dictionary to store the best split\n",
        "        best_split = {}\n",
        "        # initialise the maximum information gain with -infinity\n",
        "        max_info_gain = -float(\"inf\")\n",
        "        \n",
        "        # loop over all the attributes\n",
        "        for attribute_index in range(num_attributes):\n",
        "            attribute_values = dataset[:, attribute_index]\n",
        "            possible_splitting_values = np.unique(attribute_values)\n",
        "            # loop over all the attribute values present in the data to determine the best splitting value\n",
        "            for value in possible_splitting_values:\n",
        "                # split the dataset\n",
        "                dataset_left, dataset_right = self.split(dataset, attribute_index, value)\n",
        "                # check if there are any more child nodes at all\n",
        "                if len(dataset_left)>0 and len(dataset_right)>0:\n",
        "                    y, left_y, right_y = dataset[:, -1], dataset_left[:, -1], dataset_right[:, -1]\n",
        "                    # compute information gain\n",
        "                    curr_info_gain = self.calc_information_gain(y, left_y, right_y)\n",
        "                    # update the best split if needed\n",
        "                    if curr_info_gain>max_info_gain:\n",
        "                        best_split[\"attribute_index\"] = attribute_index\n",
        "                        best_split[\"splitting_value\"] = value\n",
        "                        best_split[\"dataset_left\"] = dataset_left\n",
        "                        best_split[\"dataset_right\"] = dataset_right\n",
        "                        best_split[\"info_gain\"] = curr_info_gain\n",
        "                        max_info_gain = curr_info_gain\n",
        "                        \n",
        "        # return best split\n",
        "        return best_split\n",
        "    \n",
        "    def split(self, dataset, attribute_index, splitting_value):\n",
        "        ''' function to split the data into two datasets'''\n",
        "        \n",
        "        dataset_left = np.array([row for row in dataset if row[attribute_index]<=splitting_value])\n",
        "        dataset_right = np.array([row for row in dataset if row[attribute_index]>splitting_value])\n",
        "        return dataset_left, dataset_right\n",
        "    \n",
        "    def calc_information_gain(self, parent, l_child, r_child):\n",
        "        ''' function to calculate the information gain '''\n",
        "        \n",
        "        weight_l = len(l_child) / len(parent)\n",
        "        weight_r = len(r_child) / len(parent)\n",
        "        gain = self.calc_entropy(parent) - (weight_l*self.calc_entropy(l_child) + weight_r*self.calc_entropy(r_child))\n",
        "        return gain\n",
        "    \n",
        "    def calc_entropy(self, dataset):\n",
        "        ''' function to calculate the entropy '''\n",
        "        \n",
        "        class_labels = np.unique(dataset)\n",
        "        entropy = 0\n",
        "        for c in class_labels:\n",
        "            p_class = len(dataset[dataset == c]) / len(dataset)\n",
        "            entropy += -p_class * np.log2(p_class)\n",
        "        return entropy\n",
        "    \n",
        "        \n",
        "    def majority_vote(self, dataset):\n",
        "        ''' function to compute value of leaf node by selecting the class that occurs most often in the leaf node'''\n",
        "        \n",
        "        Y = list(dataset)\n",
        "        max_class = max(Y, key=Y.count)\n",
        "        return max_class\n",
        "    \n",
        "    \n",
        "    def predict(self, dataset):\n",
        "        ''' function to get a predictions array of new data points '''\n",
        "        for x in dataset:\n",
        "            predictions = [self.make_prediction(x, self.root) for x in dataset]\n",
        "        return predictions\n",
        "    \n",
        "    def make_prediction(self, data_point, tree):\n",
        "        ''' recursive function to predict a single data point by following the tree from the root node to a leaf node '''\n",
        "        \n",
        "        if tree.value!=None: return tree.value\n",
        "        attribute_val = data_point[tree.attribute_index]\n",
        "        if attribute_val<=tree.splitting_value:\n",
        "            return self.make_prediction(data_point, tree.left)\n",
        "        else:\n",
        "            return self.make_prediction(data_point, tree.right)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73387104",
      "metadata": {
        "id": "73387104"
      },
      "source": [
        "# Decision Tree Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23661ac3",
      "metadata": {
        "id": "23661ac3"
      },
      "source": [
        "## 0. Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "id": "737c67aa",
      "metadata": {
        "id": "737c67aa"
      },
      "outputs": [],
      "source": [
        "\n",
        "# min number of samples in a node in order to allow splitting\n",
        "min_samples_split=5 \n",
        "# max depth of the tree\n",
        "max_depth=50\n",
        "# set random seed to make results reproducable\n",
        "random.seed(11)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b71ca3b",
      "metadata": {
        "id": "4b71ca3b"
      },
      "source": [
        "## 1. Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49b9e93a",
      "metadata": {
        "id": "49b9e93a"
      },
      "source": [
        "### 1.1 Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47ec8833",
      "metadata": {},
      "source": [
        "#### Hand Gesture Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "id": "e92a8145",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e92a8145",
        "outputId": "5883a615-2252-4bc7-bccb-5ebedb0f2d37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2910, 65) (2903, 65) (2943, 65) (2922, 65)\n"
          ]
        }
      ],
      "source": [
        "#load the four datasets sorted by label\n",
        "csv_0=pd.read_csv('./0.csv', header=None)\n",
        "csv_1=pd.read_csv('./1.csv', header=None)\n",
        "csv_2=pd.read_csv('./2.csv', header=None)\n",
        "csv_3=pd.read_csv('./3.csv', header=None)\n",
        "\n",
        "# the first 64 columns contain measurement values of the muscle activity from the sensors\n",
        "# the last column contains the label (rock - 0, scissors - 1, paper - 2, ok - 3)\n",
        "print(csv_0.shape, csv_1.shape, csv_2.shape, csv_3.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79f181f7",
      "metadata": {},
      "source": [
        "#### Iris Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "id": "a718a5e8",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(150, 5)"
            ]
          },
          "execution_count": 173,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "col_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'type']\n",
        "iris_data = pd.read_csv(\"iris.csv\", skiprows=1, header=None, names=col_names)\n",
        "iris_data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79fe2071",
      "metadata": {
        "id": "79fe2071"
      },
      "source": [
        "### 1.2 Prepare Data for Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "id": "d91751eb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d91751eb",
        "outputId": "58f79efb-ccce-4d76-a7ee-acb8e162b86d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(7006, 64) (7006, 1) (2336, 64) (2336, 1) (2336, 64) (2336, 1)\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.float64'>\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, X_val, Y_train, Y_test, Y_val = prepare_data(csv_0, csv_1, csv_2, csv_3)\n",
        "print(X_train.shape,Y_train.shape,X_val.shape, Y_val.shape, X_test.shape, Y_test.shape)\n",
        "print(type(X_train))\n",
        "print(type(X_train[1,1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e645eeb",
      "metadata": {
        "id": "9e645eeb"
      },
      "source": [
        "## 2. Training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75712092",
      "metadata": {
        "id": "75712092"
      },
      "source": [
        "### 2.1 Create Classifier "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "id": "ec2b926e",
      "metadata": {
        "id": "ec2b926e"
      },
      "outputs": [],
      "source": [
        "# create a Decision Tree Classifier instance using the Hyperparameters\n",
        "classifier = DecisionTreeClassifier(min_samples_split, max_depth)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f01cafde",
      "metadata": {
        "id": "f01cafde"
      },
      "source": [
        "### 2.2 Build Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "id": "4f9d6b7f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "4f9d6b7f",
        "outputId": "bb23b8b4-006a-43a7-f4d6-155105b82ac7"
      },
      "outputs": [],
      "source": [
        "# Build the best classification tree based based on the training and validation data\n",
        "X_train = np.concatenate((X_train, X_val), axis=0)\n",
        "Y_train = np.concatenate((Y_train, Y_val), axis=0)\n",
        "train_dataset = np.concatenate((X_train, Y_train), axis=1)\n",
        "classifier.root = classifier.build_tree_ID3(train_dataset)\n",
        "print(type(classifier.root))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c2d60a1",
      "metadata": {
        "id": "5c2d60a1"
      },
      "source": [
        "## 3. Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f90c524",
      "metadata": {
        "id": "9f90c524"
      },
      "outputs": [],
      "source": [
        "# predict gesture classes of input data using the created decision tree\n",
        "Y_pred = classifier.predict(X_test)\n",
        "Y_pred_train = classifier.predict(X_train[:1000,:]) "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "174446eb",
      "metadata": {
        "id": "174446eb"
      },
      "source": [
        "## 4. Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec938f02",
      "metadata": {
        "id": "ec938f02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The accuracy score on the test data is: 0.7675513698630136\n",
            "The accuracy score on the train data is: 0.84\n"
          ]
        }
      ],
      "source": [
        "# calculate the accuracy score of the predictions\n",
        "acc = accuracy_score(Y_test, Y_pred)\n",
        "acc_train = accuracy_score(Y_train[:1000], Y_pred_train)\n",
        "\n",
        "print(\"The accuracy score on the test data is: \" + str(acc))\n",
        "print(\"The accuracy score on the train data is: \" + str(acc_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca324b32",
      "metadata": {},
      "source": [
        "## Iris Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81e45eec",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The accuracy score with the Iris dataset on the test data is: 0.9\n",
            "The accuracy score with the Iris dataset on the train data is: 0.9666666666666667\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, X_val, Y_train, Y_test, Y_val = prepare_data(iris_data)\n",
        "classifier_iris = DecisionTreeClassifier(min_samples_split, max_depth)\n",
        "iris_train_dataset = np.concatenate((X_train, Y_train), axis=1)\n",
        "classifier_iris.root = classifier_iris.build_tree_ID3(iris_train_dataset)\n",
        "Y_pred = classifier_iris.predict(X_test)\n",
        "Y_pred_train = classifier_iris.predict(X_train[:1000,:]) \n",
        "acc = accuracy_score(Y_test, Y_pred)\n",
        "acc_train = accuracy_score(Y_train[:1000], Y_pred_train)\n",
        "print(\"The accuracy score with the Iris dataset on the test data is: \" + str(acc))\n",
        "print(\"The accuracy score with the Iris dataset on the train data is: \" + str(acc_train))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.6 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "c8e63aa0cab361309dbca6cdb7e4eacdd0c3d72b8dc44414b8c692f4ca28a130"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
